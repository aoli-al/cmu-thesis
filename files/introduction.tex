\chapter{Introduction}

The traditional view of applications as sequential and monolithic no longer applies to cloud applications, which require designs that meet evolving demands for scalability and reliability. Cloud applications operate in dynamic environments that involve distributed architectures, heterogeneous components, and complex execution patterns. While programming paradigms such as proactive error handling, distributed programming, and concurrent programming have existed for some time, their role in cloud environments has expanded and evolved significantly.


As cloud applications grow more complex, the limitations of traditional program analysis techniques for debugging and testing become evident. These techniques often fail to address the complexity of cloud systems due to three key reasons. First, they are not designed to handle the new behaviors, such as long chains of error-handling flows through different components. Second, these techniques struggle to capture the concurrency and non-determinism introduced by complex thread and message scheduling, which are central to cloud application behavior. Finally, they rely on assumptions of homogeneity across system components, ignoring the reality that cloud systems often comprise services implemented in different languages and frameworks. 


This disconnect highlights the need to rethink how program analysis is applied in cloud environments. Traditional techniques, which prioritize precision and accuracy, often do so at the cost of scalability and applicability. Cloud applications demand a shift in focus: \emph{we must embrace program analysis methods that are scalable and capable of handling the diversity and dynamism of cloud workloads without sacrificing applicability in real-world scenarios.}

These technological advances create a new challenge in applying traditional programming analysis techniques for debugging and testing as they often fall short in addressing the complexity of modern cloud applications because: 1) they are not designed to handle the new behaviors in cloud environments (e.g., long error-handling chains); 2) they assume homogeneity across components (e.g., all components are implemented in the same language); and 3) they struggle with new programming behaviors introduced by complex thread and message scheduling. Thus, design decisions for traditional applications do not transfer to cloud applications.

In this dissertation, we argue that achieving practical dynamic program analyses for applications requires a fundamental re-evaluation of classical approaches. By prioritizing scalability and applicability alongside traditional concerns of precision and accuracy, we can better address the unique challenges posed by cloud computing. We propose new methodologies designed to adapt to the evolving landscape of cloud-based systems, ultimately enabling more effective debugging and testing in modern cloud environments. 


\section{Practical Dynamic Program Analyses for Heterogeneous Cloud Applications}

Broadly, this dissertation addresses the challenges of debugging and testing cloud applications from three distinct angles. First, we design new algorithms to help developers identify the root causes of exception-dependent failures—issues are particularly difficult to diagnose in cloud environments due to complex failure handling logic leading to the spatial and temporal separation between the root cause exception and the ultimate system failure. Second, we tackle the scalability limitations of existing concurrency testing algorithms, which help us to find new bugs in well-tested industrial cloud applications. Finally, we confront one of the fundamental barriers to testing cloud applications: the lack of adequate platform and tool support. 

\subsection{Root Cause Diagnosis for Exception-dependency Failures}

Error handling is a critical aspect of building resilient cloud applications, allowing developers to create systems that continue to function even in the face of expected hardware or software failures. However, improper error handling is a well-known cause of system failures, especially in cloud environments.

We introduce the term exception-dependent failure (EDF) to describe a class of failures involving multiple exceptions, where the handling periods of the exceptions do not overlap---meaning they do not belong to the same static exception chain. Despite this, the improper handling or mismanagement of one root cause exception can propagate to a downstream exception, ultimately triggering a system-wide failure.

Exception-dependent failures are widespread in cloud applications. A prior study has demonstrated that nearly 92\% of catastrophic cloud failures stem from incorrect handling of non-fatal errors~\cite{Ding14-testfailure}. Complementing these findings, our manual analysis of 150 failures across various Apache Foundation projects reveals that 85\% of the failures can be traced back to exceptions.

Diagnosing exception-dependent failures presents significant challenges due to three primary factors. First, \emph{implicit stateful dependencies} complicate tracing the failure’s origin, as the root cause exception may not only alter the program’s control flow but also subtly modify its state in ways that are not immediately apparent. Second, \emph{silent handling} occurs when the root cause exception is either suppressed or not logged, making it difficult to detect and trace. Finally, \emph{the spatial and temporal separation} of events further obscures diagnosis, as the root cause exception may be both physically distant in code and temporally delayed from the resulting failure, making it harder to correlate the two events.



Chapter~\ref{chapter:exchain} presents {\exchain}, a tool that enables developers to diagnose exception-dependent failures by automatically inferring exception dependency. {\exchain} works by instrumenting the production system binary. At run time,  {\exchain} routines, which were instrumented into the production binary, automatically log all exceptions and their context and perform a dynamic-static hybrid analysis to identify \emph{causal dependencies} among all exceptions; e.j., an exception $e_2$ casually depends on an exception $e_1$ if $e_1$ is responsible for $e_2$, either explicitly (e.g., throw $e_2$ inside a catch block of $e_1$) or implicitly (e.g., $e_1$ changes application stats that cause $e_2$). Whenever a failure occurs, developers can query the exception-dependency graph produced by {\exchain} to see whether the mishandling of an exception caused the failure. 

\subsection{Adaptive Schedule Generation for Distributed Systems}

One of the primary challenges in testing cloud applications arises from their increasingly scalable and distributed architectures. While such architectures enable higher scalability and performance, they also introduce complexities in testing, as the system’s behavior is influenced not only by user inputs but also by event ordering.

To address this, researchers and practitioners have proposed design testing~\cite{Desai13-P, Leslie94-tla}, a methodology that allows developers to specify the core algorithms and protocols of a distributed system using modeling languages. These designs can then be automatically tested to validate that they meet certain specifications or invariants.

In testing frameworks for distributed system designs, these systems are often modeled as a collection of communicating finite-state machines, allowing the framework to fully control the ordering of how messages between machines are sent and received. This control enables developers to systematically explore the behavior of the distributed system under various \emph{schedules}—i.e., sequences of events or messages processed by the system. Since the choice of schedule can significantly influence the observed system behavior, different \emph{scheduling algorithms} can be employed to generate schedules. The \emph{search space} of the scheduling algorithm refers to the set of all possible interleavings of events that the system can undergo. 

Ideally, we could validate the properties by testing all possible schedules. The only way to do this exhaustively is via systematic testing. However, the search space is usually too large, and developers often have time and computing constraints. Thus, given a fixed time budget, the state-of-the-art approaches for distributed system design testing are {\RSBT} ({\RSBTShort}) algorithms~\cite{Yuan18-POS,Burckhardt10-PCT,Yuan20-morpheus} such as probabilistic concurrency testing (PCT)~\cite{Burckhardt10-PCT} and partial order sampling (POS)~\cite{Yuan18-POS}. These techniques operate by randomly sampling within a \emph{bounded search space} so that they focus on the executions that are more likely to cover unique behaviors compared to an unconstrained search (in a grey rectangle). The bounded search space is restricted using \emph{scheduling constraints}. For instance, PCT limits the number of interleavings or context switches, while POS focuses on sampling interleavings of events with partial order dependencies. In practice, developers have found such techniques are more effective in finding bugs as well. 

Unfortunately, a limitation of existing {\RSBTShort} algorithms is that they do not explicitly maximize behavioral coverage. Because such techniques use random sampling to generate schedules, they often repeatedly explore the same kinds of behaviors many times within the time budget while missing critical corner cases. This inefficiency arises because randomization alone does not understand which schedules are new or more likely to expose bugs, similar to the blackbox fuzzing in sequential programs~\cite{Bohme16-aflfast}. So, when developers use these tools in practice, they are left unsatisfied at the end of the time budget as the techniques do not provide confidence about how much of the search space is tested.

Thus, we aim to keep the scheduling constraints introduced by {\RSBTShort} algorithms while explicitly maximizing the behavioral coverage. To achieve this, we propose integrating feedback-guided adaptive schedule generation into the existing {\RSBTShort} algorithms instead of relying on random sampling. With adaptive scheduling, the {\RSBTShort} algorithm is guided by feedback from previously explored schedules to prioritize the generation of schedules more likely to uncover new behaviors.

Inspired by grey-box fuzzing~\cite{Bohme16-aflfast}, one way to achieve adaptive schedule generation is to mutate previously explored schedules based on execution feedback. However, applying this idea to distributed system testing presents two key challenges: (1) Directly mutating a schedule, such as altering the execution order between machines, can produce infeasible schedules, such as causing a machine to become unschedulable (e.g., being blocked from receiving a necessary message). Additionally, such mutations may violate the scheduling constraints of {\RSBTShort} algorithms, for instance, by exceeding the permitted number of context switches. (2) Given the vast search space of possible schedules, the generation algorithm must effectively identify and prioritize schedules that are more likely to uncover unique behaviors.

Chapter~\ref{chapter:fest} presents {\fest}, the first distributed design testing technique aimed at maximizing behavioral coverage. {\fest} addresses the challenges above with two innovative designs. First, {\fest} integrates record-and-replay~\cite{Ochallahan17-rr} techniques with parametric input generation~\cite{Padhye19-zest}. It records the execution of {\RSBTShort} algorithms and applies small, controlled mutations to the recorded execution. These mutated recordings are then replayed to generate new schedules that are similar yet distinct from the original, facilitating adaptive schedule generation. To tackle the second challenge—identifying schedules likely to uncover unique behaviors—{\fest} introduces a \emph{diversity feedback} mechanism that evaluates the uniqueness of a schedule’s behavior. This evaluation is based on abstract Lamport timelines~\cite{Meng23-Mallory, Lamport78-lamporttimeline}, which capture the causal relationships between system events. {\fest} prioritizes the generation of schedules with higher diversity scores. 

\subsection{General-purpose Testing and Debugging Platforms}

Despite significant advancements in algorithms and methodologies for testing and debugging cloud applications, their adoption in the industry remains limited. Many research tools claim general applicability but often leave practical engineering work as an extension, resulting in artifacts that are seldom utilized by practitioners.  This gap highlights the need to explore what critical components are missing in the development of a general-purpose testing and debugging platform for cloud applications. To address this, we conduct two case studies that aim to uncover these deficiencies and propose solutions.


\subsubsection{An Efficient General-purpose Concurrency Testing Platform for the JVM}

Our first step is to design and implement an efficient general-purpose concurrency testing platform for the JVM. Software testing is the predominant form of validating correctness for large real-world programs due to its simplicity, wide-spread applicability, efficiency, and reproducibility. However, testing \emph{multi-threaded} programs remains challenging in practice, despite the fact that concurrency bugs are among the most difficult to detect and diagnose~\cite{Lu08-bug-study, Musuvathi08-chess}. 


Take Java, which by several metrics is the most popular programming language with native support for concurrent multi-threading~\cite{pl-stackoverflow, pl-github, pl-tiobe}. Let's assume that programmers write test cases to validate the correctness of concurrent programs via simple assertions (e.g., that a \emph{parallel-sort} operation produces a sorted list). How can we check such properties? In practice, a common approach appears to be simply re-running concurrent tests multiple times~\cite{baeldung-testing-multithreaded} to check whether some assertion fails. This approach is neither efficient nor reproducible.

An ideal concurrency testing system would (a) provide an efficient mechanism for \emph{controlled concurrency}~\cite{Thomson16-sctbench}; that is, to deterministically execute a multi-threaded program along a fixed  \emph{schedule} (i.e., sequence of thread interleavings), and (b) provide support for systematically or randomly exploring thread schedules using state-of-the-art search strategies (e.g., \emph{partial order sampling}~\cite{Yuan18-POS}) to uncover hard-to-find concurrency bugs. Yet, no such general testing framework for the JVM currently exists, despite decades of research on \emph{algorithms} for concurrency testing. This is not merely due to a lack of engineering effort, but rather, it stems from fundamental limitations of prior work, which we identify in this paper.

Consider that to achieve \emph{controlled concurrency}, two contrasting approaches currently exist: (1) at the lowest level of abstraction, OS-level thread scheduling decisions can be recorded and/or manipulated by intercepting system calls, as done by tools such as rr~\cite{Ocallahan17-rr} for Linux and CHESS~\cite{Musuvathi08-chess} for Win32; (2) at the highest level of abstraction, programming-language-level threads and synchronization primitives can be completely replaced in order to take full control of concurrent execution semantics---this approach is embodied by tools such as Java Path Finder (JPF)~\cite{Visser03-jpf}, Lincheck~\cite{Koval23-lincheck}, and Coyote~\cite{Deligiannis23-coyote} (for C\#). Our observation is that both these extremes on the spectrum of abstraction levels are limited because they must make sub-optimal trade-offs (ref. Fig.~\ref{fig:abstraction-tradeoff}), as follows.



\section{Thesis Statement}

\section{Contributions}